{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building a Multilayer Perceptron with PySpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOfOW2qFbpVrfibQqvIcA4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruizleandro/Deep-Learning-PySpark/blob/master/Building_a_Multilayer_Perceptron_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADoLoyg6xaj2",
        "colab_type": "text"
      },
      "source": [
        "When it comes to using Deep Learning in Spark, there are multiple options. Depending on the exact requirement and available infrastructure, the relevant approach can be used. On a high level, there are close to 4 important deep learning libraries which can be used with Spark:\n",
        "\n",
        "1. Spark's MLlib\n",
        "2. TensorflowOnSpark\n",
        "3. Deep Learning Pipelines\n",
        "4. DeepLearning4J\n",
        "\n",
        "For simplicity, we will build a multilayer perceptron, using Spark. The dataset that we are going to use for this exercise contains close to 75k records, with some sample customer journey data on a retail web site. There are 16 input features to predict whether the visitor is likely to convert. We have a balanced target class in this dataset. We will use `MultilayerPerceptronClassifier` from Spark's ML library. We start by importing a few important dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnu0dc5bydPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('deep_learning').getOrCreate()\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtehSyjfytAq",
        "colab_type": "text"
      },
      "source": [
        "Now we load the dataset into Spark, for feature engineering and model training. As mentioned, there are 16 input features and 1 output column (`Orders_Normalized`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugSMOJeLy5RK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "c8db17ce-8114-4805-9f39-80c9b897190e"
      },
      "source": [
        "data = spark.read.csv('dl_data.csv', header=True, inferSchema=True)\n",
        "data.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Visit_Number_Bucket: string (nullable = true)\n",
            " |-- Page_Views_Normalized: double (nullable = true)\n",
            " |-- Orders_Normalized: integer (nullable = true)\n",
            " |-- Internal_Search_Successful_Normalized: double (nullable = true)\n",
            " |-- Internal_Search_Null_Normalized: double (nullable = true)\n",
            " |-- Email_Signup_Normalized: double (nullable = true)\n",
            " |-- Total_Seconds_Spent_Normalized: double (nullable = true)\n",
            " |-- Store_Locator_Search_Normalized: double (nullable = true)\n",
            " |-- Mapped_Last_Touch_Channel: string (nullable = true)\n",
            " |-- Mapped_Mobile_Device_Type: string (nullable = true)\n",
            " |-- Mapped_Browser_Type: string (nullable = true)\n",
            " |-- Mapped_Entry_Pages: string (nullable = true)\n",
            " |-- Mapped_Site_Section: string (nullable = true)\n",
            " |-- Mapped_Promo_Code: string (nullable = true)\n",
            " |-- Maped_Product_Name: string (nullable = true)\n",
            " |-- Mapped_Search_Term: string (nullable = true)\n",
            " |-- Mapped_Product_Collection: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g8HrsdezPx9",
        "colab_type": "text"
      },
      "source": [
        "We change the name of the label column from `Orders_Normalized` to `label`, to be able to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJIf1B3rzVvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "d66670f5-155d-4b41-b2b9-56eeb1f32471"
      },
      "source": [
        "data = data.withColumnRenamed('Orders_Normalized', 'label')\n",
        "data.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Visit_Number_Bucket: string (nullable = true)\n",
            " |-- Page_Views_Normalized: double (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            " |-- Internal_Search_Successful_Normalized: double (nullable = true)\n",
            " |-- Internal_Search_Null_Normalized: double (nullable = true)\n",
            " |-- Email_Signup_Normalized: double (nullable = true)\n",
            " |-- Total_Seconds_Spent_Normalized: double (nullable = true)\n",
            " |-- Store_Locator_Search_Normalized: double (nullable = true)\n",
            " |-- Mapped_Last_Touch_Channel: string (nullable = true)\n",
            " |-- Mapped_Mobile_Device_Type: string (nullable = true)\n",
            " |-- Mapped_Browser_Type: string (nullable = true)\n",
            " |-- Mapped_Entry_Pages: string (nullable = true)\n",
            " |-- Mapped_Site_Section: string (nullable = true)\n",
            " |-- Mapped_Promo_Code: string (nullable = true)\n",
            " |-- Maped_Product_Name: string (nullable = true)\n",
            " |-- Mapped_Search_Term: string (nullable = true)\n",
            " |-- Mapped_Product_Collection: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ5IL5kBzdxz",
        "colab_type": "text"
      },
      "source": [
        "Because we are dealing with both numerical and categorical coluns, we must write a pipeline to create features combinind both for model training. Therefore, we import `Pipeline`, `VectorAssembler`, and `OneHotEncoder`, to create feature vectors. We will also import `MultiClassificationEvaluator` and `MultilayerPerceptron`, to check the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-trBqY-Kz1ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf, StringType\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHAT-tYaipIU",
        "colab_type": "text"
      },
      "source": [
        "# Split into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEAXBYT0LVJ",
        "colab_type": "text"
      },
      "source": [
        "We now split the data into train, test, and validation sets, for training of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fjrFp4D0RoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, validation, test = data.randomSplit([0.7, 0.2, 0.1], 1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3VvfNoaiuDV",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhaG_iyb0XnK",
        "colab_type": "text"
      },
      "source": [
        "We create separate lists of categorical columns and numeric columns based on datatypes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OXuQmlx0d_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_columns = [item[0] for item in data.dtypes if item[1].startswith(\n",
        "    'string')]\n",
        "numeric_columns = [item[0] for item in data.dtypes if item[1].startswith(\n",
        "    'double')]\n",
        "indexers = [StringIndexer(inputCol=column, outputCol='{0}_index'.format(\n",
        "    column)) for column in categorical_columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8C9Y3QG1D5H",
        "colab_type": "text"
      },
      "source": [
        "We now create consolidated feature vectors, using `VectorAssembler`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTrRu1f71KLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "featuresCreator = VectorAssembler(\n",
        "    inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns,\n",
        "    outputCol='features')\n",
        "layers = [len(featuresCreator.getInputCols()), 4, 2, 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-2MmYagiwWp",
        "colab_type": "text"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srHDVvRe1eRY",
        "colab_type": "text"
      },
      "source": [
        "The next step is to build the `MultilayerPerceptron` model. One can play around with different hyperparameters, such as number of layers and maxiters, to improve performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdHaep8I1qH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = MultilayerPerceptronClassifier(labelCol='label',\n",
        "                                            featuresCol='features',\n",
        "                                            maxIter=100,\n",
        "                                            layers=layers,\n",
        "                                            blockSize=128,\n",
        "                                            seed=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGuMnicy157q",
        "colab_type": "text"
      },
      "source": [
        "Now that we have defined every stage, we add all these steps to the pipeline and tun it on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoKOQ-432AhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline(stages=indexers + [featuresCreator, classifier])\n",
        "model = pipeline.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ3T81dw2HrS",
        "colab_type": "text"
      },
      "source": [
        "We now calculate the predictions of the model on train, test and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcfExU3D2MgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_output_df = model.transform(train)\n",
        "validation_output_df = model.transform(validation)\n",
        "test_output_df = model.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtN_yju-2Xhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_predictionAndLabels = train_output_df.select('prediction', 'label')\n",
        "validation_predictionAndLabels = validation_output_df.select('prediction', 'label')\n",
        "test_predictionAndLabels = test_output_df.select('prediction', 'label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTP_C4bfi0h_",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BztHU3Dg2jkV",
        "colab_type": "text"
      },
      "source": [
        "We define three different metrics, to evaluate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtx0UQWC2nJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "b4f092b0-7a6a-4c16-82b8-207c58fde07a"
      },
      "source": [
        "metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
        "for metric in metrics:\n",
        "  evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
        "  print('Train ' + metric + ' = ' + str(evaluator.evaluate(\n",
        "      train_predictionAndLabels)))\n",
        "  print('Validation ' + metric + ' = ' + str(evaluator.evaluate(\n",
        "      validation_predictionAndLabels)))\n",
        "  print('Test ' + metric + ' = ' + str(evaluator.evaluate(\n",
        "      test_predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train weightedPrecision = 0.9722605697126978\n",
            "Validation weightedPrecision = 0.9734944186485901\n",
            "Test weightedPrecision = 0.9710090865749514\n",
            "Train weightedRecall = 0.9718655625913297\n",
            "Validation weightedRecall = 0.9731379731379731\n",
            "Test weightedRecall = 0.9706199460916443\n",
            "Train accuracy = 0.9718655625913297\n",
            "Validation accuracy = 0.9731379731379731\n",
            "Test accuracy = 0.9706199460916443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZCMOqby3CKf",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the deep learning model is doing reasonably well on the test data, based on the input signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aso7iIF3kNV",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This chapter covered the internals of the basic building blocks of neural networks-artifical neurons- and the entire training process of a neural network. Different ways in which deep learning models can be constructed were mentioned, and, using Spark, a multilayer perceptron model was built."
      ]
    }
  ]
}